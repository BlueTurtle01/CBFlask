We can not use Otsu thresholding as the input image's histogram is not bimodal.


Bayesian Knowledge:
* Age of person colouring, children need thicker lines and less detail. Adults want more detail but this often causes problems with water and fur.



Title: Can Machine Learning convert photographs into colouring book images for real-world production settings?

Introduction
Many children, and in more recent times, adults enjoy colouring in as a way to relax. There are many books that contain
generic images, such as flowers, race cars, landscapes etc, but there are few services that provide personalised colouring
book images, let alone good quality ones at an affordable price. As are most problems when it comes to computer coding,
and machine learning specifically, it is theoretically possible to complete this task given infinite computing power,
infinite time, and infinite knowledge, thus we could end the paper here and agree that the aim as set out in the title of the paper is possible.
However, this is not feasible in the real-world and hence we need to set some constraints on this project.
These constraints are not chosen arbitrarily, they'll be chosen with consideration for previous knowledge that we posses.
This introduction of knowledge in the form of our constraints puts us in the realm of Bayesian Machine Learning.

Given an image we want to draw an edge at every point where two coloured areas meet. This seems intuitively easy, almost trivial,
as a human. However, when we consider this more deeply it quickly becomes apparent why for a computer this is not so easy.
A join is where 2 adjacent pixel values are different. However, in a very noisy image this proves difficult as the result
is many more edges that we require. Given that we are trying to create a simpler version of the photograph to then be
coloured in we have to be selective about which edges to keep and which to discard. We want to keep the edges that are important
, such as the outline of a face, the outline of the body, or the house, etc. However, we do not want to draw an edge across every single strand of hair -
even though there may indeed be a change in pixel value as per the original requirement to draw the edge. Here is our first
introduction of knowledge that we must inject into the problem - What are important edges?

### Insert image with all contours drawn to prove how terrible this looks ###

Our second problem is how to select the number of clusters. Later we will do this numerically by minimising an error criteria but here we will discuss the high level problem.
Each cluster is a unique colour in the final image. K-means does this iteratively until convergence using some initial starting points - often randomly chosen. However, the generic
K-means selects clusters that mathematically minimise error, and not for aesthetics. An example can be seen below where there are a lot of grey/brown tones chosen but the colours
of the dress/skirt, and the skin tone are ignored and are replaced by the more popular colours. This is the expected outcome of K-means so mechanically the algorithm is working correctly.
As we can see though the clustered output is not appealing when we look at the 2 women. Our second piece of knowledge that we must include in the process is "What are important colours?".
If a human were designing the output outline for this image we would discard a shade of brown (given that we already have a few) and chose dark blue or red for the clothes instead.
The reason this is easy for a human is because we are internally assigning a value to the aesthetics of the image and decide that the gain from improved aesthetics outweighs the increase
in mathematical error that is introduced by swapping the red/blue for the shade of brown that minimises the error. Coding "aesthetic beauty" is very difficult and to some extent subjective.

It is tempting to count the number of pixels for each RGB colour and then just select k clusters for the 10 most popular colours. However, the colour of people's eyes, lips, and
clothes will likely be lost if they do not make up the majority of the image, which seems unlikely and does not allow our algorithm to be generally applied. A partial solution to this is to separate our foreground from
the background and apply k-means and edge detection to the foreground and background separately. This does not solve the problem of defining what "important colours" are, but it does allow us to use a different number of clusters
for each section. This way the k colours chosen from the background can be different than the k colours chosen from the foreground, even if k is the same value. This allows us to still retain k clusters in the background to retain for
example, detail in the foilage which would require multiple shades or green, but also retain detail in the foreground which may require multiple shades of skin tone. We can see in <Image of sophia with 2k> that just doubling the value for
k does not solve this problem as often the result is that more shades of the most common colour are chosen and as previously assumed, this is unlikely to be the important object in the photograph, if the photograph follows conventional 1/3rd framing -
 where 1/3rd of the photograph contains the important object and the other 2/3rds form the surroundings.


error is minimised when k = the number of pixels in the image as each pixel is its own cluster.


xception was significantly slower than mobilenet and the performance was only a little better at finding the masks. Did not work well on the london scene.

Literary Review

https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr

Methods
Canny

* Change Gaussian Blur to Bilateral Filter as Bilateral preserves edges, whereas Gaussian does not.
* Included a second filter Median Blur, to remove the salt-and-pepper noise.

Thresholding


Things that did not work:
* Thresholding the gray image a second time. Did not change outcome.
* Changed from using probable background to definite background as the basis for thresholding. It often removed teeth and faces otherwise.
    Now I feed definite background mask to thresholding. Loss of probability math aspect but proof that it's not always useful.

Pipeline:
There are two main approaches to creating an outline image from our photograph. We can use Canny Edge Detection (REF), or Thresholding (REF).
There are two sub approaches as part of Thresholding that I will explore but first I will use Canny Edge Detection (Canny) so we can see the differences and discuss why
Canny may not be appropriate given our aim.


Canny works in the sense that it does indeed find the edges of the objects in the image, but, given our aim is not the most useful.
Here, we are following in the Bayes notion in that we are considering our knowledge, incorporating that into our decisions, and then acting appropriately.
Given our knowledge of the aim, to make a colouring book, it looks to be that Canny is not useful as the contours are not always connected - which is
a requirement of a colouring book - and some of the closed contours are very small. Given our prior knowledge of the age of the customer, for example children,
this may make Canny even more inappropriate for the task. Now we will look at Thresholding as a comparison.

Thresholding has two sub approaches, 1. Thresholding the whole image, 2. Thresholding the foreground and background separately.
To threshold the foreground and background individually we must first separate the foreground from the background. To do this I will use Machine Learning. Specifically I will split this approach
down further and use a Faster-RCNN to create bounding boxes around each of the detected objects in the image, and a DeepLabV3 Semantic Segmentation model that will create a mask over the object.
Both the bounding box and mask are fed into a GrabCut algorithm to be discussed later.

# Faster-RCNN


# DeepLabV3


# GrabCut: REF: https://dl.acm.org/doi/10.1145/1186562.1015720
GrabCut is an iterative algorithm that aims to classify pixels into the same class if they are part of the same object in the image. It works by estimating the colour distribution of the object and that of the
background and using a Gaussian Mixture Model to determine which pixels belong to the object and those that belong to the object. We create a Markov Random field over the pixel values conduct a graph cut
using the output prediction of which pixels are background and which are foreground. This is run until convergence.

Initialisation: The algorithm requires a bounding box or mask that contains all of the foreground objects. This will be known as the "Definite Foreground". The remainder of the image can now be ignored by the GrabCut
algorithm as we have defined this as "Definite Background". This is akin to injecting information into the model, as per the Bayesian style.

Steps:
1. A Gaussian Mixture Model is applied to the pixel values that defines each pixel as either "Probable Foreground" or "Probable Background" depending on its relation with neighbouring pixels.
2. A graph is then constructed from this pixel distribution where each pixel is assigned to a node in this connected, undirected, graph. We add 2 more nodes:
a Sink and a Source node. Each "Probable Foreground" pixel is connected to the Source node and every "Probably Background" pixel is
connected to the Sink node. These connections are determined by the probability that each pixel belongs to background or foreground.
We have previously determined "Definite Background" and "Definite Foreground" so those probabilities will be 1 for their respective Sink or Source node connection.
The edge weights for the probable pixels is initialised with a seed but some will change after each iteration, until none of the weights change
and we have convergence.

The weights between pixels is determined by the information we can gain from their pixel values. Two values that are almost identical will have similar pixel values
and we can determine that this is not an edge and hence attach a high weight to inform the algorithm that these pixels belong together.

3. A mincut algorithm (toer-Wagner Algorithm) (REF: https://dl.acm.org/doi/10.1145/263867.263872) is then used to cut the graph. (EXPAND) This algorithm cuts the graph at the point where a cost function is minimised
and assigns some pixels to the Source node and the rest to the Sink node.

The output of the GrabCut algorithm is an array containing only the pixels that are "Probably Foreground" and the remainder are black.
This becomes the input to our k-means algorithm and later Thresholding.



# Colouring

## kmeans



The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.

The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, ‘How slow is the k-means method?’ SoCG2006)


K-means is often faster than it's theoretical maximum time complexity but it can fall into local minima so it can be beneficial to restart the algorithm multiple times. The algorithm is initialised
with initial centroids that are determined either randomly or using or other algorithms such as k-means++, Forgy, or the Bradley and Fayyad approach <CITE https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.8528&rep=rep1&type=pdf>. <REF> This leads to k-means outputting different
clusters for the same input data each time the algorithm is run.